<!doctype html>
<html lang="en" dir="ltr">
  <head>
    <link href="css/style.css" rel="stylesheet" type="text/css" media="all" />
    <link rel="icon" type="image/x-icon" href="img/favicon.ico" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <title>Taishi Nakamura</title>
  </head>
  <body>
    <nav>
      <ul>
        <li><a href="#about">About</a></li>
        <li><a href="#education">Education</a></li>
        <li><a href="#experience">Experience</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#service">Service</a></li>
      </ul>
    </nav>
    <div class="centering-block">
      <div class="centering-block-inner">
        <div class="profile-header">
          <img
            src="./img/profile.jpg"
            alt="Taishi Nakamura"
            class="profile-image"
          />
          <div class="profile-info">
            <h1 id="about">Taishi Nakamura</h1>
            <a
              href="https://twitter.com/Setuna7777_2"
              target="_blank"
              rel="noreferrer"
              class="sns"
              ><img
                src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/twitter.svg"
                width="32"
                height="32"
            /></a>
            <a
              href="https://www.github.com/Taishi-N324"
              target="_blank"
              rel="noreferrer"
              ><img
                src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/github.svg"
                width="32"
                height="32"
            /></a>
            <a
              href="https://www.linkedin.com/in/taishi-nakamura"
              target="_blank"
              rel="noreferrer"
              ><img
                src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/linkedin.svg"
                width="32"
                height="32"
            /></a>
            <a
              href="https://scholar.google.com/citations?hl=en&user=nbPQwgUAAAAJ"
              target="_blank"
              rel="noreferrer"
            >
              <img
                src="./img/google-scholar.svg"
                width="32"
                height="32"
                alt="Google Scholar"
              />
            </a>
            <p>
              Email: taishi [at] rio (dot) scrc (dot) iir (dot) isct (dot) ac
              (dot) jp
            </p>
          </div>
        </div>

        <p>
          Hi! ðŸ‘‹ I'm Taishi, a second-year Master's student in the Department of
          Computer Science at Institute of Science Tokyo, advised by Professor
          <a href="https://www.rio.gsic.titech.ac.jp/en/member/yokota.html"
            >Rio Yokota</a
          >.
        </p>

        <p>
          My research focuses on scaling foundation models efficiently and
          optimizing their performance. I've been working on continual
          pre-training methods for large language models, mixture of experts
          architectures, and test-time compute approaches to enhance reasoning
          capabilities.
        </p>

        <p>
          I'm passionate about open source and have been actively contributing
          to open source projects. In building models with strong Japanese
          language capabilities from scratch, I led pre-training and ablation
          experiments for
          <a
            href="https://huggingface.co/collections/llm-jp/llm-jp-ver20-models-672c62c1b28c400174e9e5a2"
            >LLM-jp ver2.0 Models</a
          >
          and spearheaded the development of flagship
          <a
            href="https://llmc.nii.ac.jp/en/topics/release-of-llm-jp-3-moe-series/"
            >LLM-jp-3 MoE</a
          >
          under the guidance of Professor
          <a href="https://www.fai.cds.tohoku.ac.jp/members/js/">Jun Suzuki</a>.
          In enhancing Japanese language models through continual pre-training,
          I contributed to
          <a href="https://swallow-llm.github.io/index.en.html">Swallow LLM</a>
          through model evaluation, dataset construction, establishing continual
          pre-training methodologies, and model release. For open multilingual
          model development, I led the training and release of the
          <a
            href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407"
            >Aurora-M</a
          >
          model in collaboration with
          <a href="https://www.ontocord.ai/">Ontocord.ai</a> and Professor
          <a href="https://www.utu.fi/en/people/sampo-pyysalo">Sampo Pyysalo</a
          >.
        </p>

        <p>
          Prior to my current research, our team was selected for the
          prestigious
          <a
            href="https://www.ipa.go.jp/en/it-talents/mitou/target-quantum-computing-2023.html"
            >MITOU TARGET program for Quantum Computing</a
          >
          under Dr.
          <a
            href="https://www.rd.ntt/e/organization/researcher/special/s_023.html"
            >Yuuki Tokunaga</a
          >'s mentorship. We developed an educational platform for quantum
          computing that is now publicly available as the
          <a href="https://qualsimu.com/textbook/">Qualsimu Textbook</a>.
        </p>

        <h2 id="education">Education</h2>
        <p>
          <strong>Apr 2024 - Mar 2026 (expected)</strong> &nbsp; Master of
          Science in Computer Science, Institute of Science Tokyo (Formerly
          Tokyo Institute of Technology)
        </p>
        <p>
          <strong>Apr 2021 - Mar 2024</strong> &nbsp; Bachelor of Science in
          Computer Science, Tokyo Institute of Technology
        </p>

        <h2 id="experience">Experience</h2>
        <p>
          <strong>May 2024 - Present</strong> &nbsp; Research Assistant,
          National Institute of Informatics Research and Development Center for
          Large Language Models
        </p>
        <p>
          <strong>Feb 2024 - Present</strong> &nbsp; Research Intern, Sakana AI
        </p>
        <p>
          <strong>Aug 2023 - Present</strong> &nbsp; Student Trainee, National
          Institute of Advanced Industrial Science and Technology
        </p>
        <p>
          <strong>Apr 2023 - Present</strong> &nbsp; Research Assistant,
          Institute of Science Tokyo (Formerly Tokyo Institute of Technology)
        </p>
        <p>
          <strong>Oct 2023 - Apr 2024</strong> &nbsp; Research Intern, LLM-JP
        </p>
        <p>
          <strong>Jun 2023 - Feb 2024</strong> &nbsp; MITOU Target Program,
          Information-technology Promotion Agency, Japan
        </p>

        <h2 id="publications">Selected Publications</h2>

        <p>
          For a complete list of publications and full papers, please see my
          <a
            href="https://scholar.google.com/citations?hl=en&user=nbPQwgUAAAAJ"
            target="_blank"
            rel="noreferrer"
          >
            Google Scholar profile </a
          >.
        </p>

        <h3><strong>Peer-Reviewed Conference Publications</strong></h3>

        <p></p>

        <p>
          Yuichi Inoue* Kou Misaki*, Yuki Imajuku, So Kuroki,
          <strong>Taishi Nakamura</strong>, Takuya Akiba.
          <a href="https://arxiv.org/abs/2503.04412" target="_blank"
            >Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive
            Branching Tree Search.</a
          >
          Conference on Neural Information Processing Systems (NeurIPS)
          <span style="color: red">Spotlight</span>, 2025.
          <strong>*Equal contribution</strong>. [<a
            href="https://github.com/SakanaAI/treequest"
            target="_blank"
            >Code</a
          >][<a href="https://sakana.ai/ab-mcts/" target="_blank">Blog</a>]
        </p>

        <p>
          <strong>Taishi Nakamura</strong>, Takuya Akiba, Kazuki Fujii, Yusuke
          Oda, Rio Yokota, Jun Suzuki.
          <a href="https://arxiv.org/abs/2502.19261" target="_blank"
            >Drop-Upcycling: Training Sparse Mixture of Experts with Partial
            Re-initialization.</a
          >
          International Conference on Learning Representations (ICLR), 2025. [<a
            href="https://huggingface.co/collections/llm-jp/drop-upcycling-674dc5be7bbb45e12a476b80"
            target="_blank"
            >Hugging Face</a
          >][<a
            href="https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3"
            target="_blank"
            >Dataset</a
          >][<a
            href="https://github.com/Taishi-N324/Drop-Upcycling"
            target="_blank"
            >Code</a
          >][<a
            href="https://wandb.ai/taishi-nakamura/Drop-Upcycling"
            target="_blank"
            >Log</a
          >]
        </p>

        <p>
          So Kuroki, <strong>Taishi Nakamura</strong>, Takuya Akiba, Yujin Tang.
          <a href="https://arxiv.org/abs/2410.14735" target="_blank"
            >Agent Skill Acquisition for Large Language Models via CycleQD.</a
          >
          International Conference on Learning Representations (ICLR), 2025. [<a
            href="https://huggingface.co/collections/SakanaAI/cycleqd-670ef174021859516e72f7e1"
            target="_blank"
            >Hugging Face</a
          >][<a href="https://github.com/SakanaAI/CycleQD" target="_blank"
            >Code</a
          >][<a href="https://sakana.ai/cycleqd/" target="_blank">Blog</a>]
        </p>

        <p>
          <strong>Taishi Nakamura*</strong>, Mayank Mishra*, Simone Tedeschi*,
          ..., Matthew Blumberg, #Victor May, #Huu Nguyen, #Sampo Pyysalo (49
          authors).
          <a href="https://arxiv.org/abs/2404.00399" target="_blank"
            >Aurora-M: Open Source Continual Pre-training for Multilingual
            Language and Code.</a
          >
          International Conference on Computational Linguistics (COLING)
          Industry Track, 2025.
          <strong>*Equal contribution #Equal mentoring</strong>. [<a
            href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407"
            target="_blank"
            >Hugging Face</a
          >][<a
            href="https://github.com/Taishi-N324/Megatron-LM-LUMI"
            target="_blank"
            >Code</a
          >][<a
            href="https://huggingface.co/blog/mayank-mishra/aurora"
            target="_blank"
            >Blog</a
          >]
        </p>

        <p>
          Kazuki Fujii*, <strong>Taishi Nakamura*</strong>, Mengsay Loem, Hiroki
          Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio
          Yokota, Naoaki Okazaki.
          <a href="https://arxiv.org/abs/2404.17790" target="_blank"
            >Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing
            Japanese Language Capabilities.</a
          >
          Conference on Language Modeling (COLM), 2024.
          <span style="color: red"
            >Outstanding Paper Award at Annual Meeting of the Japanese
            Association for NLP. Language Resources Award at Annual Meeting of
            the Japanese Association for NLP.</span
          >
          <strong>*Equal contribution</strong>
          [<a href="https://huggingface.co/tokyotech-llm" target="_blank"
            >Hugging Face</a
          >][<a
            href="https://github.com/rioyokotalab/Megatron-Llama2"
            target="_blank"
            >Code</a
          >]
        </p>

        <h3><strong>Workshop Publications</strong></h3>

        <p>
          <strong>Taishi Nakamura</strong>, Satoki Ishikawa, Masaki Kawamura,
          Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota.
          <a href="https://arxiv.org/abs/2508.18672" target="_blank"
            >Optimal Sparsity of Mixture-of-Experts Language Models for
            Reasoning Tasks.</a
          >
          International Conference on Machine Learning (ICML) AI for Math
          Workshop 2025. [<a
            href="https://huggingface.co/collections/llm-jp/optimal-sparsity-math-68a4a5fa635fd1c1628280f1"
            target="_blank"
            >Hugging Face</a
          >][<a
            href="https://github.com/rioyokotalab/optimal-sparsity"
            target="_blank"
            >Code</a
          >]
        </p>

        <h2 id="service">Service</h2>
        <h3><strong>Reviewer</strong></h3>
        <p>ICLR 2026</p>
        <p>Workshops, ICML 2025 AI4Math, NeurIPS 2025 ER</p>

        <br />Last updated: Jan 2026
      </div>
    </div>
  </body>
</html>
