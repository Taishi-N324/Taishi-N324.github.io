<!doctype html>
<html lang="en" dir="ltr">
  <head>
    <link href="css/style.css" rel="stylesheet" type="text/css" media="all" />
    <link rel="icon" type="image/x-icon" href="img/favicon.ico" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <title>Taishi Nakamura</title>
  </head>
  <body>
    <nav>
      <ul>
        <li><a href="#about">About</a></li>
        <li><a href="#education">Education</a></li>
        <li><a href="#experience">Experience</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#service">Service</a></li>
      </ul>
    </nav>
    <div class="centering-block">
      <div class="centering-block-inner">
        <div class="profile-header">
          <img
            src="./img/profile.jpg"
            alt="Taishi Nakamura"
            class="profile-image"
          />
          <div class="profile-info">
            <h1 id="about">Taishi Nakamura</h1>
            <a
              href="https://twitter.com/Setuna7777_2"
              target="_blank"
              rel="noreferrer"
              class="sns"
              ><img
                src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/twitter.svg"
                width="32"
                height="32"
            /></a>
            <a
              href="https://www.github.com/Taishi-N324"
              target="_blank"
              rel="noreferrer"
              ><img
                src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/github.svg"
                width="32"
                height="32"
            /></a>
            <a
              href="https://www.linkedin.com/in/taishi-nakamura"
              target="_blank"
              rel="noreferrer"
              ><img
                src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/linkedin.svg"
                width="32"
                height="32"
            /></a>
            <a
              href="https://scholar.google.com/citations?hl=en&user=nbPQwgUAAAAJ"
              target="_blank"
              rel="noreferrer"
            >
              <img
                src="./img/google-scholar.svg"
                width="32"
                height="32"
                alt="Google Scholar"
              />
            </a>
            <p>
              Email: taishi [at] rio (dot) scrc (dot) iir (dot) isct (dot) ac
              (dot) jp
            </p>
          </div>
        </div>

        <p>
          Hi! ðŸ‘‹ I'm Taishi, a second-year Master's student in the Department of
          Computer Science at Institute of Science Tokyo, advised by Professor
          <a href="https://www.rio.gsic.titech.ac.jp/en/member/yokota.html"
            >Rio Yokota</a
          >.
        </p>

        <p>
          My research focuses on scaling foundation models efficiently and
          optimizing their performance. I've been working on continual
          pre-training methods for large language models, mixture of experts
          architectures, and test-time compute approaches to enhance reasoning
          capabilities.
        </p>

        <p>
          I'm passionate about open source and have been actively contributing
          to open source projects. In building models with strong Japanese
          language capabilities from scratch, I led pre-training and ablation
          experiments for
          <a
            href="https://huggingface.co/collections/llm-jp/llm-jp-ver20-models-672c62c1b28c400174e9e5a2"
            >LLM-jp ver2.0 Models</a
          >
          and spearheaded the development of flagship
          <a
            href="https://llmc.nii.ac.jp/en/topics/release-of-llm-jp-3-moe-series/"
            >LLM-jp-3 MoE</a
          >
          under the guidance of Professor
          <a href="https://www.fai.cds.tohoku.ac.jp/members/js/">Jun Suzuki</a>.
          In enhancing Japanese language models through continual pre-training,
          I contributed to
          <a href="https://swallow-llm.github.io/index.en.html">Swallow LLM</a>
          through model evaluation, dataset construction, establishing continual
          pre-training methodologies, and model release. For open multilingual
          model development, I led the training and release of the
          <a
            href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407"
            >Aurora-M</a
          >
          model in collaboration with
          <a href="https://www.ontocord.ai/">Ontocord.ai</a> and Professor
          <a href="https://www.utu.fi/en/people/sampo-pyysalo">Sampo Pyysalo</a
          >.
        </p>

        <p>
          Prior to my current research, our team was selected for the
          prestigious
          <a
            href="https://www.ipa.go.jp/en/it-talents/mitou/target-quantum-computing-2023.html"
            >MITOU TARGET program for Quantum Computing</a
          >
          under Dr.
          <a
            href="https://www.rd.ntt/e/organization/researcher/special/s_023.html"
            >Yuuki Tokunaga</a
          >'s mentorship. We developed an educational platform for quantum
          computing that is now publicly available as the
          <a href="https://qualsimu.com/textbook/">Qualsimu Textbook</a>.
        </p>

        <h2 id="education">Education</h2>
        <p>
          <strong>Apr 2024 - Present</strong> &nbsp; Master of Science in
          Computer Science, Institute of Science Tokyo (Formerly Tokyo Institute
          of Technology)
        </p>
        <p>
          <strong>Apr 2021 - Mar 2024</strong> &nbsp; Bachelor of Science in
          Computer Science, Tokyo Institute of Technology
        </p>

        <h2 id="experience">Experience</h2>
        <p>
          <strong>May 2024 - Present</strong> &nbsp; Research Assistant,
          National Institute of Informatics Research and Development Center for
          Large Language Models
        </p>
        <p>
          <strong>Feb 2024 - Present</strong> &nbsp; Research Intern, Sakana AI
        </p>
        <p>
          <strong>Aug 2023 - Present</strong> &nbsp; Student Trainee, National
          Institute of Advanced Industrial Science and Technology
        </p>
        <p>
          <strong>Apr 2023 - Present</strong> &nbsp; Research Assistant,
          Institute of Science Tokyo (Formerly Tokyo Institute of Technology)
        </p>
        <p>
          <strong>Oct 2023 - Apr 2024</strong> &nbsp; Research Intern, LLM-JP
        </p>
        <p>
          <strong>Jun 2023 - Feb 2024</strong> &nbsp; MITOU Target Program,
          Information-technology Promotion Agency, Japan
        </p>

        <h2 id="publications">Publications</h2>

        <h3><strong>Peer-Reviewed Conference Publications</strong></h3>

        <p></p>

        <p>
          Yuichi Inoue* Kou Misaki*, Yuki Imajuku, So Kuroki,
          <strong>Taishi Nakamura</strong>, Takuya Akiba.
          <a href="https://arxiv.org/abs/2503.04412" target="_blank"
            >Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive
            Branching Tree Search.</a
          >
          Conference on Neural Information Processing Systems (NeurIPS)
          <span style="color: red">Spotlight</span>, 2025.
          <strong>*Equal contribution</strong>. [<a
            href="https://github.com/SakanaAI/treequest"
            target="_blank"
            >Code</a
          >][<a href="https://sakana.ai/ab-mcts/" target="_blank">Blog</a>]
        </p>

        <p>
          Youmi Ma, Sakae Mizuki, Kazuki Fujii,
          <strong>Taishi Nakamura</strong>, Masanari Ohi, Hinari Shimada, Taihei
          Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto,
          Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki.
          <a href="https://arxiv.org/abs/2503.23714" target="_blank"
            >Building Instruction-Tuning Datasets from Human-Written
            Instructions with Open-Weight Large Language Models.</a
          >
          Conference on Language Modeling (COLM), 2025.
        </p>

        <p>
          <strong>Taishi Nakamura</strong>, Takuya Akiba, Kazuki Fujii, Yusuke
          Oda, Rio Yokota, Jun Suzuki.
          <a href="https://arxiv.org/abs/2502.19261" target="_blank"
            >Drop-Upcycling: Training Sparse Mixture of Experts with Partial
            Re-initialization.</a
          >
          International Conference on Learning Representations (ICLR), 2025. [<a
            href="https://huggingface.co/collections/llm-jp/drop-upcycling-674dc5be7bbb45e12a476b80"
            target="_blank"
            >Hugging Face</a
          >][<a
            href="https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3"
            target="_blank"
            >Dataset</a
          >][<a
            href="https://github.com/Taishi-N324/Drop-Upcycling"
            target="_blank"
            >Code</a
          >][<a
            href="https://wandb.ai/taishi-nakamura/Drop-Upcycling"
            target="_blank"
            >Log</a
          >]
        </p>

        <p>
          So Kuroki, <strong>Taishi Nakamura</strong>, Takuya Akiba, Yujin Tang.
          <a href="https://arxiv.org/abs/2410.14735" target="_blank"
            >Agent Skill Acquisition for Large Language Models via CycleQD.</a
          >
          International Conference on Learning Representations (ICLR), 2025. [<a
            href="https://huggingface.co/collections/SakanaAI/cycleqd-670ef174021859516e72f7e1"
            target="_blank"
            >Hugging Face</a
          >][<a href="https://github.com/SakanaAI/CycleQD" target="_blank"
            >Code</a
          >][<a href="https://sakana.ai/cycleqd/" target="_blank">Blog</a>]
        </p>

        <p>
          <strong>Taishi Nakamura*</strong>, Mayank Mishra*, Simone Tedeschi*,
          ..., Matthew Blumberg, #Victor May, #Huu Nguyen, #Sampo Pyysalo (49
          authors).
          <a href="https://arxiv.org/abs/2404.00399" target="_blank"
            >Aurora-M: Open Source Continual Pre-training for Multilingual
            Language and Code.</a
          >
          International Conference on Computational Linguistics (COLING)
          Industry Track, 2025.
          <strong>*Equal contribution #Equal mentoring</strong>. [<a
            href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407"
            target="_blank"
            >Hugging Face</a
          >][<a
            href="https://github.com/Taishi-N324/Megatron-LM-LUMI"
            target="_blank"
            >Code</a
          >][<a
            href="https://huggingface.co/blog/mayank-mishra/aurora"
            target="_blank"
            >Blog</a
          >]
        </p>

        <p>
          Kazuki Fujii*, <strong>Taishi Nakamura*</strong>, Mengsay Loem, Hiroki
          Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio
          Yokota, Naoaki Okazaki.
          <a href="https://arxiv.org/abs/2404.17790" target="_blank"
            >Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing
            Japanese Language Capabilities.</a
          >
          Conference on Language Modeling (COLM), 2024.
          <span style="color: red"
            >Outstanding Paper Award at Annual Meeting of the Japanese
            Association for NLP. Language Resources Award at Annual Meeting of
            the Japanese Association for NLP.</span
          >
          <strong>*Equal contribution</strong>
          [<a href="https://huggingface.co/tokyotech-llm" target="_blank"
            >Hugging Face</a
          >][<a
            href="https://github.com/rioyokotalab/Megatron-Llama2"
            target="_blank"
            >Code</a
          >]
        </p>

        <p>
          Naoaki Okazaki, Kakeru Hattori, Hirai Shota, Hiroki Iida, Masanari
          Ohi, Kazuki Fujii, <strong>Taishi Nakamura</strong>, Mengsay Loem, Rio
          Yokota, Sakae Mizuki.
          <a href="https://arxiv.org/abs/2404.17733" target="_blank"
            >Building a Large Japanese Web Corpus for Large Language Models.</a
          >
          Conference on Language Modeling (COLM), 2024.
          <span style="color: red"
            >Outstanding Paper Award at Annual Meeting of the Japanese
            Association for NLP.</span
          >
        </p>

        <h3><strong>Workshop Publications</strong></h3>

        <p>
          Koshiro Saito, Sakae Mizuki, Masanari Ohi,
          <strong>Taishi Nakamura</strong>, Taihei Shiotani, Koki Maeda, Youmi
          Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida,
          Hiroya Takamura, Rio Yokota, Naoaki Okazaki.
          <a href="https://arxiv.org/abs/2412.14471" target="_blank"
            >Why We Build Local Large Language Models: An Observational Analysis
            from 35 Japanese and Multilingual LLMs.</a
          >
          Conference on Language Modeling (COLM) Multilingual and Equitable
          Language Technologies Workshop 2025.
          <span style="color: red"
            >Outstanding Paper Award, Natural Language Processing Research
            Meeting of the Information Processing Society of Japan.</span
          >
        </p>
        <p>
          <strong>Taishi Nakamura</strong>, Satoki Ishikawa, Masaki Kawamura,
          Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota.
          <a href="https://arxiv.org/abs/2508.18672" target="_blank"
            >Optimal Sparsity of Mixture-of-Experts Language Models for
            Reasoning Tasks.</a
          >
          International Conference on Machine Learning (ICML) AI for Math
          Workshop 2025.
        </p>

        <p>
          Kazuki Fujii,
          <strong>Taishi Nakamura</strong>, Rio Yokota. llm-recipes: A Framework
          for Seamless Integration and Efficient Continual Pre-Training of Large
          Language Models. The International Conference for High Performance
          Computing, Networking, Storage, and Analysis (SC) Trillion Parameter
          Consortium Workshop, 2024. [<a
            href="https://tpc.dev/wp-content/uploads/2024/11/sc24-tpc-workshop-FUJII.pdf"
            target="_blank"
            >Slide</a
          >] [<a href="https://github.com/okoge-kaz/llm-recipes" target="_blank"
            >Code</a
          >]
        </p>

        <h3><strong>Preprints</strong></h3>

        <p>
          Marianna Nezhurina, JÃ¶rg Franke, <strong>Taishi Nakamura</strong>,
          Timur Carstensen, NiccolÃ² Ajroldi, Ville Komulainen, David Salinas,
          Jenia Jitsev.
          <a href="https://arxiv.org/abs/2509.09009" target="_blank"
            >Open-sci-ref-0.01: open and reproducible reference baselines for
            language model and dataset comparison.</a
          >
          arXiv:2509.09009 [cs.LG], 2025. [<a
            href="https://huggingface.co/collections/open-sci/open-sci-ref-001-685905e598be658fbcebff4f"
            target="_blank"
            >Hugging Face</a
          >]
        </p>

        <p>
          Kazuki Fujii, Yukito Tajima, Sakae Mizuki, Hinari Shimada, Taihei
          Shiotani, Koshiro Saito, Masanari Ohi, Masaki Kawamura,
          <strong>Taishi Nakamura</strong>, Takumi Okamoto, Shigeki Ishida,
          Kakeru Hattori, Youmi Ma, Hiroya Takamura, Rio Yokota, Naoaki Okazaki.
          <a href="https://arxiv.org/abs/2505.02881" target="_blank"
            >Rewriting Pre-Training Data Boosts LLM Performance in Math and
            Code.</a
          >
          arXiv:2505.02881 [cs.LG], 2025. [<a
            href="https://huggingface.co/tokyotech-llm"
            target="_blank"
            >Hugging Face</a
          >][<a
            href="https://github.com/rioyokotalab/swallow-code-math"
            target="_blank"
            >Code</a
          >]
        </p>

        <p>
          Kazuki Fujii, <strong>Taishi Nakamura</strong>, Rio Yokota.
          <a href="https://arxiv.org/abs/2411.08719" target="_blank"
            >Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16
            Training in LLMs.</a
          >
          arXiv:2411.08719 [cs.LG], 2024.
        </p>

        <p>
          LLM-jp.
          <a href="https://arxiv.org/abs/2407.03963" target="_blank"
            >LLM-jp: A Cross-organizational Project for the Research and
            Development of Fully Open Japanese LLMs.</a
          >
          arXiv:2407.03963 [cs.CL], 2024. Authors are listed in alphabetical
          order. [<a href="https://huggingface.co/llm-jp" target="_blank"
            >Hugging Face</a
          >][<a
            href="https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2"
            target="_blank"
            >Dataset</a
          >][<a href="https://github.com/llm-jp" target="_blank">Code</a>]
        </p>

        <h2 id="service">Service</h2>
        <h3><strong>Reviewer</strong></h3>
        <p>Workshops, ICML 2025 AI4Math, NeurIPS 2025 ER</p>

        <br />Last updated: Sep 2025
      </div>
    </div>
  </body>
</html>
